{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic programming with PyMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc as mc\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import scipy.stats\n",
    "from pydataset import data\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (6.0, 3.0)\n",
    "mpl.rcParams['figure.dpi'] = 150\n",
    "\n",
    "## Utility function to plot the graph of a PyMC model\n",
    "def show_dag(model):\n",
    "    dag = mc.graph.dag(model)\n",
    "    dag.write(\"graph.png\",format=\"png\")\n",
    "    from IPython.display import Image\n",
    "    i = Image(filename='graph.png')\n",
    "    return i\n",
    "\n",
    "mpl.style.use('ggplot')\n",
    "np.random.seed(2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neat things\n",
    "\n",
    "* Demo of MCMC algorithms and their sampling properties: https://chi-feng.github.io/mcmc-demo/\n",
    "* Hamiltoninan MCMC visually explained (great animations): http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html\n",
    "* [A Conceptual Introduction to Hamiltonian Monte Carlo](https://arxiv.org/abs/1701.02434) An excellent paper on the theory of Hamiltonian Monte Carlo sampling\n",
    "* [Introduction to MCMC](http://www.inference.org.uk/mackay/erice.pdf) by David Mackay\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic purpose\n",
    "We will cover probabilistic **inference**. Rather than learning a single set of parameters by optimisation, we can model probability distributions over possible models that might be compatible with our data.  We'll use Monte Carlo sampling to make it simple and easy (if not very efficient) to work with probabilistic models. \n",
    "\n",
    "\n",
    "MCMC models:\n",
    "\n",
    "* **Data**, which we observe as a collection of examples.\n",
    "* A **model** which has **structure** (a DAG) and **parameters**\n",
    "* Part of the model is a likelihood function which has \"contact\" with data; these we will call **observed random variables**\n",
    "* Part of the model specifies distributions over parameters of the **observed variables**. These are **unobserved random variables**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyMC\n",
    "<a id=\"pymc\"> </a>\n",
    "We'll use the excellent PyMC module to do the inference. If you have questions about this module, you can read [this tutorial](http://arxiv.org/abs/1507.08050) or the [API docs](https://pymc-devs.github.io/pymc/). \n",
    "\n",
    "# Fitting a normal distribution\n",
    "## Bayesian Normal fitting\n",
    "We use Monte Carlo sampling to estimate the mean and standard deviation of some data. We generate some synthetic data from a known normal distribution \n",
    "$$x \\sim \\mathcal{N}(-1, 1.5)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate data with a known distribution\n",
    "## this will be our \"observed\" data\n",
    "x_data = np.random.normal(-1,1.5, (3000,))\n",
    "\n",
    "plt.hist(x_data, bins=np.linspace(-5,5,15))\n",
    "plt.title(\"Histogram of data\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a model in PyMC. We have a single output variable `x`, which is **stochastic** and **observed**, and the data we have observed is `x_data`. As it is observed, we will use the likelihood of the data under different model settings to accept/reject samples in the process.\n",
    "\n",
    "We have a model:\n",
    "\n",
    "$$\n",
    "\\mu \\sim \\mathcal{N}(0, 10^2)\\\\\n",
    "\\tau \\sim \\mathcal{\\Gamma}(2.0, 20.0)\\\\\n",
    "x\\sim\\mathcal{N}(\\mu, \\frac{1}{\\tau})\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent variables\n",
    "mu = mc.Normal('mu', mu=0, tau=1.0/(10*10))  # wide range for prior on means\n",
    "prec = mc.Gamma('prec', alpha=2.0, beta=20)         # wide range for prior on precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "xs = np.linspace(0, 100, 100)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "# alpha = 1.0, beta = 20.0\n",
    "ax.plot(xs, scipy.stats.gamma(a=2.0, scale=20).pdf(xs))\n",
    "ax.set_xlabel(\"$\\\\tau$\")\n",
    "ax.set_ylabel(\"$p(\\\\tau)$\")\n",
    "ax.set_title(\"Precision $\\\\tau$ prior\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observed variable\n",
    "observed_stochastic = mc.Normal('observed', \n",
    "                                 mu=mu, tau=prec, \n",
    "                                 observed=True, \n",
    "                                 value=x_data)\n",
    "\n",
    "# if we want to sample from unconditioned prior\n",
    "#observed_stochastic = mc.Normal('observed',\n",
    "#mu=mu, tau=prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to set parents for every node. In this case, we have two parameters, $\\mu$ and $\\tau = \\frac{1}{\\sigma^2}$ to specify ($\\tau$ is used to make it easier to parameterise priors over normals). We want to infer those, so we also make those stochastic variables, but unobserved (hidden). We specify the type of the distribution (here, `Normal` and `Uniform`) and we must then specify *those* parents. In this case, these are just concrete numbers (but we could go further if we wanted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate samples with same distribution\n",
    "# here, we draw 20 samples in each sample\n",
    "pred_posterior = mc.Normal('predictive', \n",
    "                           mu=mu, \n",
    "                           tau=prec, \n",
    "                           size=20)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add a \"false\" variable that will be used to make draws from the predictive posterior. It is a variable with the same parents as the observed posterior, but unobserved. Here we generate 20 posterior predictive samples for every accepted MCMC sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the graphical model\n",
    "model = mc.Model([mu, prec, observed_stochastic, \n",
    "                  pred_posterior])\n",
    "show_dag(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile the model and show the graph. We can now draw samples from it, discarding the first portion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the distribution\n",
    "mcmc = mc.MCMC(model)\n",
    "mcmc.sample(iter=50000, burn=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **trace** is the collection of posterior samples, as a straightforward array. We can plot these using the built in visualisation tool:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard trace plot\n",
    "mc.Matplot.plot(mcmc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access them directly as arrays and plot them more flexibly (including showing draws from the predictive posterior):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_hist(trace, name):\n",
    "        n, bins, patches = plt.hist(trace, normed=True, bins=50)                \n",
    "        max_n = np.max(n)\n",
    "        plt.title(\"Estimate of {var_name}\".format(var_name=name))\n",
    "        \n",
    "        # draw simple statistics\n",
    "        ctr_max = 0.5 * (bins[np.argmax(n)] + bins[np.argmax(n)+1])\n",
    "        plt.axvline(ctr_max, ls='-', color='r', lw=2, label='MAP')\n",
    "        plt.axvline(np.median(trace), ls='-', color='C1', lw=2, label='Median')\n",
    "        plt.axvline(np.mean(trace), ls=':', color='k', label='Expected')\n",
    "        # 90% credible interval\n",
    "        plt.axvline(np.percentile(trace, 5.0), ls=':', color='C1')\n",
    "        plt.axvline(np.percentile(trace, 95.0), ls=':', color='C1')\n",
    "        plt.fill_between(x=[np.percentile(trace, 5.0), np.percentile(trace, 95.0)], y1=max_n,\n",
    "                          color='C1', alpha=0.2, label='90% credible')\n",
    "        plt.text(np.mean(trace), 0.5*max_n, 'Mean')\n",
    "        plt.legend()\n",
    "        plt.gca().set_frame_on(False)\n",
    "        \n",
    "def show_trace(mcmc, vars_):\n",
    "    ## plot histograms of possible parameter values\n",
    "    # from the trace\n",
    "    for var,name in vars_.items():\n",
    "        plt.figure()\n",
    "        trace = mcmc.trace(var)[:].ravel()\n",
    "        trace_hist(trace, name)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "show_trace(mcmc, {\"mu\":\"mean\", \n",
    "                  \"prec\":\"precision\",\n",
    "                  \"predictive\":\"predictive posterior\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes to try:\n",
    "\n",
    "* Show prior predictive (remove `observed=`)\n",
    "* Show fixing \\mu and \\tau to known values\n",
    "* Adjust $n$ to show effect of prior/posterior\n",
    "* Show fitting a uniform distribution instead (below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent variables\n",
    "x_data = np.random.uniform(-2, 3, size=800)\n",
    "\n",
    "np.random.seed(21225)\n",
    "# Note: I *force* a good starting point (i.e. non-zero probability)\n",
    "# otherwise PyMC will not be able to form the model\n",
    "# this is why value=0 and value=100 are used\n",
    "\n",
    "ctr = mc.Normal('ctr', mu=0, tau=1e-2, \n",
    "                )  # wide range for prior on means\n",
    "width = mc.Gamma('width', alpha=2.0, \n",
    "                 beta=200.0, value=100.0)         # wide range for prior on precision\n",
    "\n",
    "\n",
    "\n",
    "# uniform variable, in a unknown range\n",
    "# note use of Lambda to transform parameters\n",
    "lower = mc.Lambda('upper', lambda ctr=ctr, \n",
    "                  width=width: ctr-width)\n",
    "upper = mc.Lambda('lower', lambda ctr=ctr, \n",
    "                  width=width: ctr+width)\n",
    "\n",
    "observed_stochastic = mc.Uniform('observed', \n",
    "                                 upper=upper, \n",
    "                                 lower=lower, \n",
    "                                 observed=True, \n",
    "                                 value=x_data)\n",
    "\n",
    "pred_posterior = mc.Uniform('predictive', \n",
    "                            upper=upper, \n",
    "                            lower=lower)\n",
    "\n",
    "# display the graphical model\n",
    "model = mc.Model([ctr, width, \n",
    "                  observed_stochastic, \n",
    "                  pred_posterior, upper, lower])\n",
    "show_dag(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc = mc.MCMC(model)\n",
    "# run 10 independent chains, so we can measure convergence\n",
    "for i in range(10):\n",
    "    mcmc.sample(iter=50000, burn=1000, thin=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic convergence statistics\n",
    "### Gelman-Rubin\n",
    "Measures intra-chain versus inter-chain variance (should be similar if mixing is good)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closer to 1.0 is better (means within chain variance is close to across chain \n",
    "# variance)\n",
    "mc.gelman_rubin(mcmc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rafferty-Lewis\n",
    "Estimates the burn-in and thinning required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will print results for all 10 chains\n",
    "mc.raftery_lewis(mcmc, q=0.025, r=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trace(mcmc, {\"ctr\":\"centre\", \"width\":\"width\", \"predictive\":\"Predictive posterior\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.Matplot.plot(mcmc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical models\n",
    "<a id=\"graphical\"> </a>\n",
    "\n",
    "Transformations of expressions to graphs is familiar to most computer scientists -- it is an essential part of most optimising compilers. For example, the equation of a straight line might be written as a graph (this is how a compiler would break down the expression):\n",
    "\n",
    "<img src=\"imgs/ymxc.png\" width=\"300px\">\n",
    "\n",
    "## Adding unknowns\n",
    "If we have multiple dependent random variables whose distribution we want to infer, we can draw a graph of dependencies to form a *graphical model*.  This explictly models dependencies between **random variables** (i.e. ones we don't know the value of precisely) and inference can be performed on the entire graph. \n",
    "\n",
    "**In CS terms, we are writing expressions down without fixing the variables, and then allowing the distribution of the values to be inferred when we observe data.** This inference process narrows down the likely range a random variable could take on (hopefully!).\n",
    "\n",
    "In a **probabilistic graphical model**, some nodes in the graph are **observed** -- that is we know their state because we have explicity measured it, and others are **unobserved** -- we know (or have guessed) the form of their distribution but not the parameters of that distribution. Some dependencies are deterministic (i.e. fully defined by the values of their parents), while others are stochastic. We can infer the **posterior** distribution of unobserved nodes by integrating over the possible values that could have occured given the observed values.\n",
    "\n",
    "We can modify our straight line equation to write a model for **linear regression**:\n",
    "\n",
    "<img src=\"imgs/ymxc_stochastic.png\">\n",
    "\n",
    "All we need to do is specify that we expected the output $y$ to be normally distributed around the equation of a line given by $m$ and $c$; we can now **infer** $\\sigma, m, c$ from observed data. Or we can fix any of them, and infer the remainder (if, e.g. we knew in advance that $c=0$). Our assumption here is that we will observe data which has a **latent structure** modelled by a linear dependence on a variable $x$, plus some normally-distributed observation noise.\n",
    "\n",
    "**Note that we must put *some* prior distribution on every stochastic node and we can only observe stochastic nodes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the linear regression model in the intro in practice, using PyMC to build a graphical model and then run MCMC to sample from the posterior (i.e. estimate the distribution of random variables after seeing some evidence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bayesian Linear Regression with pymc\n",
    "### We use Monte Carlo sampling to estimate the distribution of a linear function with a normally\n",
    "### distributed error, given some observed data.\n",
    "### Vaguely based on: http://matpalm.com/blog/2012/12/27/dead_simple_pymc/ and http://sabermetricinsights.blogspot.co.uk/2014/05/bayesian-linear-regression-with-pymc.html\n",
    "\n",
    "\n",
    "## generate data with a known distribution\n",
    "## this will be our \"observed\" data\n",
    "x = np.sort(np.random.uniform(0,20, (50,)))\n",
    "m = 2\n",
    "c = 15\n",
    "\n",
    "# Add on some measurement noise, with std. dev. 3.0\n",
    "epsilon = data = np.random.normal(0, 3, x.shape)\n",
    "y = m * x + c + epsilon\n",
    "\n",
    "plt.plot(x,y, '.', label=\"Datapoints\")\n",
    "plt.plot(x, m*x+c, '--', lw=3, label=\"True\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\")\n",
    "plt.xlabel(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now, set up the PyMC model\n",
    "\n",
    "## specify the prior distribution of the unknown line function variables\n",
    "## Here, we assume a normal distribution over m and c\n",
    "m_unknown = mc.Normal('m', 0, 0.01)\n",
    "c_unknown = mc.Normal('c', 0, 0.001)\n",
    "\n",
    "## specify a prior over the precision (inverse variance) of the error term\n",
    "# precision = 1/variance\n",
    "## Here we specify a uniform distribution from 0.001 to 10.0\n",
    "precision = mc.Uniform('precision', \n",
    "                       lower=0.001, \n",
    "                       upper=10.0)\n",
    "\n",
    "# this is just a convenience for plotting\n",
    "std_dev = mc.Lambda(\"std_dev\", \n",
    "                    lambda precision=precision: \n",
    "                    np.sqrt(1.0/precision))\n",
    "\n",
    "# specify the observed input variable\n",
    "# we use a normal distribution, but this has no effect --\n",
    "# the values are fixed and the parameters\n",
    "# never updated; this is just a way of transforming x \n",
    "# into a variable pymc can work with\n",
    "# (it's really a hack)\n",
    "x_obs = mc.Normal(\"x_obs\", 0, 1,\n",
    "                  value=x, observed=True)\n",
    "\n",
    "@mc.deterministic(plot=False)\n",
    "def line(m=m_unknown, c=c_unknown, x=x_obs):\n",
    "    return x*m+c\n",
    "\n",
    "# specify the observed output variable \n",
    "y_obs =  mc.Normal('y_obs', mu=line, \n",
    "                   tau=precision, \n",
    "                   value=y, \n",
    "                   observed=True\n",
    "                  )\n",
    "\n",
    "model = mc.Model([m_unknown, c_unknown, precision, x_obs, y_obs, std_dev])\n",
    "\n",
    "# display the graphical model\n",
    "show_dag(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the distribution\n",
    "mcmc = mc.MCMC(model)\n",
    "mcmc.sample(iter=50000, burn=1000, thin=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trace(mcmc, {\"m\":\"m\", \n",
    "                  \"c\":\"c\", \n",
    "                  \"std_dev\":\"std_dev\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draws from the posterior predictive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[<img src=\"https://imgs.xkcd.com/comics/error_bars.png\">](https://xkcd.com/2110)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now plot overlaid samples from the linear function\n",
    "## Note: this *ignores* the error distribution we've estimated\n",
    "## If we drew samples from the true posterior predictive, \n",
    "# we'd see much greater spread\n",
    "## in possible simulations\n",
    "ms = mcmc.trace(\"m\")[:]\n",
    "cs = mcmc.trace(\"c\")[:]\n",
    "\n",
    "plt.title(\"Sampled fits\")\n",
    "plt.plot(x, y, '.', label=\"Observed\")\n",
    "\n",
    "xf = np.linspace(-20,40,200)\n",
    "for m,c in zip(ms[::20], cs[::20]):    \n",
    "    plt.plot(xf, xf*m+c, 'g-', alpha=0.01)\n",
    "plt.plot(x, x*m+c, '--', label=\"True\", zorder=100)\n",
    "plt.legend()\n",
    "plt.xlim(-20,40)\n",
    "plt.ylim(-40,80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple diagnostics\n",
    "Is our sampler taking uncorrelated samples? We can look at the **autocorrelation** of the samples. If they are perfectly unbiased, then this should be zero everywhere (no correlation between successive samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mc.Matplot.autocorrelation(mcmc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Sampling issues\n",
    "## Burn-in and thinning\n",
    "\n",
    " The **great thing** about MCMC approaches is that you can basically write down your model and then run inference directly. There is no need to derive complex approximations, or to restrict ourselves to limited models for which we can compute answers analytically. Essentially, no maths by hand; everything is done algorithmically.\n",
    "\n",
    "The **bad thing** about MCMC approaches is that, even though it will do the \"right thing\" *asymptotically*, the choice of sampling strategy has a very large influence for the kind of sample runs that are practical to execute. Bayesian inference should depend only on the priors and the evidence observed; but MCMC approaches also depend on the sampling strategy used to approximate the posterior. \n",
    "\n",
    "### Dealing with biased sampling\n",
    "MCMC tries to draw **independent, unbiased** samples from the posterior, but the sampling process (like Metropolis), is not inherently unbiased. For example, successive samples in a random walk are correlated and obviously not independent. \n",
    "\n",
    "And although the Markov Chain approach (under fairly relaxed assumptions) will asympotically sample from all of the posterior, if the random walk starts off very far from the bulk of the distribution, it will \"wander in the wilderness\" for some time before reaching significant probability density. This means early samples from the distribution might be unreasonably dense in very low probability regions in the posterior. How \"good\" the Markov chain is at sampling from the posterior is called **mixing**; some MCMC setups may mix very badly until they get warmed up.\n",
    "\n",
    "To mitigate these two common issues, there are a couple of standard tricks: \n",
    "* **Burn-in**, which ignores the first $n$ samples from an MCMC draw, to make sure the chain is \"mixing\" well. Typically, several thousand samples might be ignored.\n",
    "* **Thinnning**, which takes one sample from every $k$ consecutive samples from the chain, to reduce correlation. Values of raound 5-50 are common.\n",
    "\n",
    "Tuning these is a matter of art!\n",
    "\n",
    "The code below implements M-H sampling from the lecture notes, and then shows how burn-in and thinning can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis(fx, q, x_init,n):\n",
    "    # Perform Metropolis MCMC sampling.\n",
    "    # p(x): a function that can be evaluated anywhere. p(x) returns the value of p at x\n",
    "    # q(): a function q that draws a sample from a symmetric distribution and returns it\n",
    "    # x_init: a starting point\n",
    "    # n: number of samples\n",
    "    x = x_init\n",
    "    \n",
    "    samples = []\n",
    "    rejected = [] # we only keep the rejected samples to plot them later\n",
    "    for i in range(n):\n",
    "        # find a new candidate spot to jump to\n",
    "        x_prime =  q(x)        \n",
    "        p_r = fx(x_prime)/fx(x)\n",
    "        r = np.random.uniform(0,1) \n",
    "        # if it's better, go right away\n",
    "        if r<p_r:            \n",
    "            x = x_prime\n",
    "            samples.append(x_prime)\n",
    "        else:\n",
    "            rejected.append(x_prime)            \n",
    "                \n",
    "    return np.array(samples), np.array(rejected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "# don't worry about how this code works\n",
    "# test the sampling process\n",
    "# create an interesting distribution p (just a mixture of two gaussians)\n",
    "A = np.array([[0.15, 0.9], [-0.3, 2.5]])\n",
    "p1 = lambda x:scipy.stats.multivariate_normal(mean=[0,0], cov=A).pdf(x)\n",
    "p2 = lambda x:scipy.stats.multivariate_normal(mean=[3,0], cov=np.eye(2)).pdf(x)\n",
    "p = lambda x:p1(x)*0.5+p2(x)*0.5\n",
    "# create a proposal distribution, with std. dev. 0.5\n",
    "q = lambda x: np.random.normal(x,0.5,(2,))\n",
    "\n",
    "# make 500 MCMC steps\n",
    "accept, reject = metropolis(p,q,[10.5, 30], 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a heatmap of the distribution, along with the\n",
    "# accepted and rejected samples from that MCMC chain\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(accept[:,0], accept[:,1], label=\"Path\", lw=0.4)\n",
    "ax.plot(accept[:,0], accept[:,1], 'b.', label='Accepted', markersize=1)\n",
    "ax.plot(reject[:,0], reject[:,1], 'rx', label='Rejected', markersize=1)\n",
    "ax.legend()\n",
    "x,y = np.meshgrid(np.linspace(-5,5,30), np.linspace(-4,4,30))\n",
    "img = ax.imshow(p(np.dstack([x,y])), extent=[-4,4,-4,4], cmap='viridis', origin='lower')\n",
    "ax.grid(\"off\")\n",
    "fig.colorbar(img, ax=ax, fraction=0.046, pad=0.04)\n",
    "ax.set_title(\"MCMC sampling with Metropolis-Hastings\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Burn-in and thinning plot\n",
    "\n",
    "# introduce correlations\n",
    "y = accept[:,1] \n",
    "x = np.arange(len(y))\n",
    "\n",
    "# discard 400 samples, keep every 8th sample\n",
    "burn = 400\n",
    "thin = 8\n",
    "plt.plot(x[0:burn], y[0:burn], 'r:')\n",
    "plt.plot(x[burn::thin], y[burn::thin], 'go', markersize=1)\n",
    "plt.plot(x[burn:], y[burn:], 'k:', alpha=0.1, markersize=0.2)\n",
    "plt.plot(x[burn:], y[burn:], 'k.', alpha=0.1, markersize=0.2)\n",
    "\n",
    "\n",
    "plt.axvline(burn, c='r')\n",
    "plt.text(15,2.5,\"Burn-in period\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red samples are discarded during burn-in, and the green samples (thinned to every 8th sample) are kept during the remainder of the sampling process. This helps to draw unbiased samples from the posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The PyMC manual](https://pymc-devs.github.io/pymc/modelchecking.html) explains a number of other diagnostic statistics and plots. **None of these are definitive**, but can give skilled MCMC practitioners insight into the operation of the sampling process.\n",
    "If you're interested in leaning more about MCMC, David Mackay's [book chapter](http://www.inference.phy.cam.ac.uk/mackay/itprnn/ps/356.384.pdf) is a good reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\n",
    "# Logistic regression example: discrete dependent variable\n",
    "On ye olde iris dataset, using the four flower measurements to predict whether or not\n",
    "the species is `setosa` or another type of iris.\n",
    "\n",
    "We estimate a set of coefficients $\\beta_0, \\beta_1, \\dots$ and use the logistic function to transform the a linear model into a probability for a Bernoulli variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pydataset import data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = data(\"iris\")\n",
    "iris[\"is_setosa\"] = np.where(iris[\"Species\"]==\"setosa\", 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a train and test set\n",
    "iris_train, iris_test = train_test_split(iris)\n",
    "print(\"Train size\", iris_train.shape)\n",
    "print(\"Test size\", iris_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model:\n",
    "\n",
    "We have some coefficients $\\beta$, which feed into our logistic function to produce $l$, and $y$ is Bernoulli distributed (0 or 1) with probability $l$.\n",
    "\n",
    "$$\n",
    "\\beta_i \\sim \\mathcal{N}(0, 5)\\\\\n",
    "l = \\frac{1}{1+e^{\\beta_0 + \\sum_i \\beta_i x_i}}\\\\\n",
    "y \\sim \\mathcal{B}(l)\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary prediction of \"is_setosa\", using the four attributes\n",
    "# of the flower configuration\n",
    "\n",
    "# predictors (standardised)\n",
    "xs = np.array(iris_train.iloc[:, 0:4])\n",
    "x_standardised = (xs - xs.mean()) / xs.std()\n",
    "\n",
    "# observed values\n",
    "ys = np.array(iris_train[\"is_setosa\"])\n",
    "\n",
    "# PyMC variable for inputs\n",
    "x_std = mc.Normal(\"x_std\", 0, 1, \n",
    "                  value=x_standardised, \n",
    "                  observed=True)\n",
    "\n",
    "# 4 regression coefficients\n",
    "betas = mc.Normal(\"betas\", mu=0, tau=1.0/(50*50), \n",
    "                  size=5, value=[0,0,0,0,0])\n",
    "\n",
    "# link function\n",
    "@mc.deterministic\n",
    "def logistic(betas=betas, x_std=x_std):\n",
    "    return 1.0 / (1 + np.exp(-(betas[0] \n",
    "                               + np.sum(betas[1:] * x_std))))\n",
    "\n",
    "# observed output is Bernoulli distributed\n",
    "y = mc.Bernoulli(\"y\", p=logistic, \n",
    "                 observed=True, value=ys)\n",
    "\n",
    "model = mc.Model([x_std, y,betas, logistic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_dag(model)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc = mc.MCMC(model)\n",
    "\n",
    "## Run the sampler with 5 different chains\n",
    "mcmc.sample(iter=150000, burn=10000, thin=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,12))\n",
    "for i in range(5):\n",
    "    ax = fig.add_subplot(3,2,i+1)    \n",
    "    trace_hist(mcmc.trace(\"betas\")[:,i], \"$\\\\beta_{i}$\".format(i=i))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "# write link function for use in prediction\n",
    "def logistic_predict(betas, x_std):\n",
    "    return 1.0 / (1 + np.exp(-(betas[0] + np.sum(betas[1:] * x_std, axis=1))))\n",
    "\n",
    "# standardise predictors in test set\n",
    "test_x = iris_test.iloc[:, 0:4]\n",
    "test_x = (test_x - np.mean(test_x))/np.std(test_x)\n",
    "\n",
    "y_true = iris_test[\"is_setosa\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "We can draw samples from the posterior and then use the regression coefficients to make new predictions. Annoyingly, \n",
    "we have to rewrite the logistic function, but this is easy to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for true versus predicted\n",
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(1,1,1)\n",
    "ax1.set_xlabel(\"True\")\n",
    "ax1.set_ylabel(\"Predicted\")\n",
    "ax1.set_title(\"True versus predicted\")\n",
    "\n",
    "# plot for ROC curve\n",
    "fig2 = plt.figure()\n",
    "ax2 = fig2.add_subplot(1,1,1)\n",
    "ax2.set_xlabel(\"FPR\")\n",
    "ax2.set_xlabel(\"TPR\")\n",
    "\n",
    "confusions = []\n",
    "beta_trace = mcmc.trace(\"betas\")[:]\n",
    "\n",
    "# predict \n",
    "for i in range(6):\n",
    "    # choose a random set of betas\n",
    "    beta_ix = np.random.randint(0, beta_trace.shape[0]-1)\n",
    "    beta_vec = beta_trace[beta_ix, :]        \n",
    "    y_pred =  logistic_predict(beta_vec, test_x)    \n",
    "    ax1.scatter(y_true+np.random.normal(0,0.01,\n",
    "                                        y_true.shape),\n",
    "                y_pred,s=0.2)\n",
    "    # bias is due to unbalanced classes (I think)\n",
    "    y_class = np.where(y_pred<0.5, 0, 1)\n",
    "    confusion = sklearn.metrics.confusion_matrix(y_true, y_class)    \n",
    "    confusions.append(confusion)\n",
    "        \n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_true, y_pred)\n",
    "    ax2.plot(fpr, tpr)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of confusion matrices\n",
    "We can show the (samples from) distribution of confusion matrices if we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.imshow(np.mean(confusions, axis=0))\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.imshow(np.std(confusions, axis=0))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show samples from the confusion matrices\n",
    "confusions = np.array(confusions)\n",
    "# some tensor reshaping fun...\n",
    "confusion_pad = np.stack([confusions, np.zeros_like(confusions)]).swapaxes(0,1)\n",
    "flat = np.concatenate(np.concatenate(confusion_pad, axis=0), axis=1)\n",
    "plt.imshow(flat, cmap='magma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Switchpoint model: more complex logic\n",
    "\n",
    "<img src=\"poverty_rates.png\">\n",
    "\n",
    "*[Source: https://ourworldindata.org/extreme-history-methods]*\n",
    "\n",
    "Data not provided, so hand-digitised via https://apps.automeris.io/wpd/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# load data from a string\n",
    "\n",
    "data = StringIO(\"\"\"year,poverty_rate\n",
    "1819.8097502972653, 83.88791593695271\n",
    "1849.6789536266351, 81.646234676007\n",
    "1869.655172413793, 75.48161120840629\n",
    "1889.821640903686, 71.6987740805604\n",
    "1909.6076099881093, 65.67425569176883\n",
    "1928.8228299643283, 56.42732049036777\n",
    "1949.7502972651605, 54.8861646234676\n",
    "1959.6432818073722, 44.09807355516638\n",
    "1969.7265160523186, 35.69176882661996\n",
    "1979.8097502972653, 31.62872154115587\n",
    "1991.6052318668253, 23.782837127845866\n",
    "2004.922711058264, 13.695271453590195\n",
    "2001.8787158145064, 17.19789842381782\n",
    "1999.0249702734839, 19.159369527145344\n",
    "1995.9809750297266, 19.299474605954472\n",
    "1987.0392390011891, 24.483362521891436\n",
    "1989.8929845422117, 24.483362521891436\n",
    "1983.9952437574316, 27.98598949211906\n",
    "1980.9512485136743, 33.450087565674266\n",
    "1992.936979785969, 22.521891418563897\"\"\")\n",
    "\n",
    "poverty_ = pd.read_csv(data)\n",
    "# deleting the dodgy data point\n",
    "# uncomment to experiment\n",
    "# poverty = poverty_.drop(labels=[6])\n",
    "\n",
    "poverty = poverty_\n",
    "poverty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poverty.plot(x='year', y='poverty_rate', kind='scatter')\n",
    "plt.gca().set_frame_on(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "We model the data with a linear regression, but where there is a switchpoint, where the regression coefficient changes (i.e. piecewise linear with two pieces). We estimate both the regression coefficients at each position and the location of the switchpoint.\n",
    "\n",
    "$$s \\sim \\mathcal{N}{(1960, 100)}\\\\\n",
    "\\beta_0 \\sim \\mathcal{N}(50, 10)\\\\\n",
    "\\beta_1 \\sim \\mathcal{N}(-1, 2)\\\\\n",
    "\\beta_2 \\sim \\mathcal{N}(-1, 2)\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu = \\begin{cases}\n",
    "x<s & \\beta_0 + \\beta_1 (x-s)\\\\\n",
    "x>s & \\beta_0 + \\beta_2 (x-s)\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\tau \\sim \\mathcal{\\Gamma}(1, 10) \\\\\n",
    "y \\sim \\mathcal{N}(\\mu, \\frac{1}{\\tau})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PyMC variable for inputs\n",
    "x = mc.Normal(\"x\", mu=0, tau=1, \n",
    "              observed=True, value=poverty[\"year\"])\n",
    "\n",
    "# 3 betas\n",
    "beta_0 = mc.Normal(\"beta_0\", mu=50, tau=1.0/(10.0))\n",
    "\n",
    "beta_1 = mc.Normal(\"beta_1\", mu=-1, tau=1.0/(2*2))\n",
    "beta_2 = mc.Normal(\"beta_2\", mu=-1, tau=1.0/(2*2))\n",
    "\n",
    "precision = mc.Gamma(\"precision\", alpha=0.5, beta=1)\n",
    "# alternatively, could postulate uniform\n",
    "#switch = mc.Uniform(\"switch\", lower=1820, upper=2020)\n",
    "switch = mc.Normal(\"switch\", mu=1935, \n",
    "                   tau=1.0/(50.0*50.0))\n",
    "\n",
    "# link function\n",
    "@mc.deterministic\n",
    "def switch_mu(beta_0=beta_0, beta_1=beta_1, \n",
    "              beta_2=beta_2,\n",
    "              x=x, switch=switch):\n",
    "    \n",
    "    return np.where(x<switch, \n",
    "                    beta_0 + beta_1 * (x-switch),\n",
    "                    beta_0 + beta_2 * (x-switch))\n",
    "\n",
    "std = mc.Lambda(\"std_dev\", \n",
    "                lambda precision=precision:np.sqrt(1.0/precision))\n",
    "\n",
    "y = mc.Normal(\"y\", mu=switch_mu, tau=precision, \n",
    "              observed=True, \n",
    "              value=poverty[\"poverty_rate\"])\n",
    "\n",
    "model = mc.Model([x, y, precision, beta_0, \n",
    "                  beta_1, beta_2, switch, switch_mu, std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_dag(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc = mc.MCMC(model)\n",
    "mcmc.sample(iter=200000, burn=2000, thin=20)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trace(mcmc, {\"beta_0\":\"intercept\", \n",
    "                  \"beta_1\":\"slope left\",\n",
    "                  \"beta_2\":\"slope right\",\n",
    "                  \"switch\":\"switchpoint (year)\",\n",
    "                 \"std_dev\":\"std. dev.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poverty.plot(x='year', y='poverty_rate', kind='scatter')\n",
    "ax = plt.gca()\n",
    "ax.set_frame_on(False)\n",
    "ax.set_ylabel(\"Global poverty rate\")\n",
    "ax.set_xlim(1800,2020)\n",
    "ax.set_ylim(0,100)\n",
    "\n",
    "\n",
    "beta_0_trace = mcmc.trace(\"beta_0\")[:]\n",
    "beta_1_trace = mcmc.trace(\"beta_1\")[:]\n",
    "beta_2_trace = mcmc.trace(\"beta_2\")[:]\n",
    "switch_trace = mcmc.trace(\"switch\")[:]\n",
    "tau_trace = mcmc.trace(\"precision\")[:]\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    ix = np.random.randint(0, beta_0_trace.shape[0]-1)\n",
    "    s = switch_trace[ix]    \n",
    "    \n",
    "    tau = tau_trace[ix]\n",
    "    x1 = np.clip(s-200, 1800, 2020)\n",
    "    x2 = s\n",
    "    x3 = np.clip(s+200, 1800, 2020)\n",
    "    y1 = beta_0_trace[ix] + beta_1_trace[ix] * (x1-s)\n",
    "    y2 = beta_0_trace[ix] + beta_1_trace[ix] * (x2-s)\n",
    "    y3 = beta_0_trace[ix] + beta_2_trace[ix] * (x3-s)\n",
    "    ax.plot([x1,x2,x3], [y1,y2,y3], 'k', lw=0.05)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# A simple mixture model: discrete + continuous latent variables\n",
    "## When things get tricky\n",
    "\n",
    "We can include both **discrete** and **continuous** variables. A very important case is where we have a **mixture model**. That is, we believe our observations come from one of a number of distributions. For example, in modelling human heights, we might expect height to be normally distributed, but to have two different distributions for men and women.\n",
    "\n",
    "<img src=\"imgs/mixture.png\">\n",
    "\n",
    "It is very straightforward to add this to a PyMC graphical model; it is just another random variable to infer. However, sampling is another matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adapted from the example given at \n",
    "## http://stackoverflow.com/questions/18987697/how-to-model-a-mixture-of-3-normals-in-pymc\n",
    "\n",
    "np.random.seed(2028)\n",
    "n = 3\n",
    "ndata = 100\n",
    "\n",
    "\n",
    "## Generate synthetic mixture-of-normals data, \n",
    "# with means at -50,0,+50, and std. dev of 5,10,1\n",
    "v = np.random.randint( 0, n, ndata)\n",
    "data = ((v==0)*(np.random.normal(50,5,ndata)) + \n",
    "        (v==1)*(np.random.normal(-50,10,ndata)) + \n",
    "        (v==2)*np.random.normal(0,1,ndata))\n",
    "\n",
    "\n",
    "## Plot the original data\n",
    "plt.hist(data, bins=50);  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A Dirichlet distribution specifies the distribution over categories\n",
    "## All 1 means that every category is equally likely\n",
    "dd = mc.Dirichlet('dd', theta=(1,)*n)\n",
    "\n",
    "## This variable \"selects\" the category (i.e. the normal distribution)\n",
    "## to use. The Dirichlet distribution sets the prior over the categories.\n",
    "category = mc.Categorical('category', \n",
    "                          p=dd, size=ndata)\n",
    "\n",
    "## Now we set our priors the precision and mean of each normal distribution\n",
    "## Note the use of \"size\" to generate a **vector** of variables \n",
    "# (i.e. one for each category)\n",
    "\n",
    "## We expect the precision of each normal to be Gamma distributed \n",
    "# (this mainly forces it to be positive!)\n",
    "precs = mc.Gamma('precs', alpha=1, \n",
    "                 beta=10, size=n)\n",
    "\n",
    "## And the means of the normal to be normally distributed, with a precision of 0.001 \n",
    "# (i.e. std. dev 1000)\n",
    "means = mc.Normal('means', 0, 1.0/(100*100), size=n)\n",
    "\n",
    "## These deterministic functions link the means of the observed distribution \n",
    "# to the categories\n",
    "## They just select one of the elements of the mean/precision vector, \n",
    "# given the current value of category\n",
    "## The input variables must be specified in the parameters, so that \n",
    "# PyMC knows which variables to pass to it\n",
    "@mc.deterministic\n",
    "def mean(category=category, means=means):\n",
    "    return means[category]\n",
    "\n",
    "@mc.deterministic\n",
    "def prec(category=category, precs=precs):\n",
    "    return precs[category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we specify the variable we observe -- which is normally distributed, *but*\n",
    "## we don't know the mean or precision. \n",
    "# Instead, we pass the **functions** mean() and pred()\n",
    "## which will be used at each sampling step.\n",
    "## We specify the observed values of this node, and tell PyMC these are observed \n",
    "## This is all that is needed to specify the model\n",
    "obs = mc.Normal('obs', mean, prec, \n",
    "                value=data, observed = True)\n",
    "\n",
    "## Now we just bundle all the variables together for PyMC\n",
    "model = mc.Model({'dd': dd,\n",
    "              'category': category,\n",
    "              'precs': precs,\n",
    "              'means': means,\n",
    "              'obs': obs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "show_dag(model)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc = mc.MCMC(model)\n",
    "\n",
    "## Now we tell the sampler what method to use\n",
    "## Metropolis works well, but we must tell PyMC to use a specific\n",
    "## discrete sampler for the category variable to get good results in a reasonable time\n",
    "mcmc.use_step_method(mc.AdaptiveMetropolis, \n",
    "                     model.means)\n",
    "mcmc.use_step_method(mc.AdaptiveMetropolis,\n",
    "                     model.precs)\n",
    "mcmc.use_step_method(mc.DiscreteMetropolis, \n",
    "                     model.category) ## this step is key!\n",
    "mcmc.use_step_method(mc.AdaptiveMetropolis, \n",
    "                     model.dd)\n",
    "\n",
    "## Run the sampler with 5 different chains\n",
    "mcmc.sample(iter=150000, burn=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(mcmc.trace('means', chain=None).gettrace()[:], normed=True, bins=np.linspace(-100,100,50))\n",
    "plt.title(\"Estimated means\")\n",
    "plt.legend(['Component 1', 'Component 2', 'Component 3'])\n",
    "plt.figure()\n",
    "## show the result in terms of std. dev. (i.e sqrt(1.0/precision))\n",
    "plt.title(\"Estimated std. dev\")\n",
    "plt.hist(np.sqrt(1.0/mcmc.trace('precs', chain=None).gettrace()[:]), normed=True, \n",
    "         bins=np.linspace(0,15,50))\n",
    "plt.legend(['Component 1', 'Component 2', 'Component 3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation in quadratic regression\n",
    "<a id=\"imputation\"> </a>\n",
    "\n",
    "In PyMC, variables can be **observed** (fixed) or **unobserved** (random). PyMC cycles through the array of known values for the **observed** variables and updates the rest of the graph.\n",
    "\n",
    "\n",
    "PyMC implements this using **imputation**, where certain missing values in an observed variable can be inferred (*imputed*) from the rest of the model. This creates new random variables and then infers the missing values. **Masked arrays** are used to implement imputation; these allow arrays to have \"blank\" values, that PyMC can fill in automatically.\n",
    "\n",
    "This approach creates one new random variable per missing data item; this can create very large models if you are not careful!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example, using very simple quadratic regression model \n",
    "import numpy.ma as ma # masked array support\n",
    "\n",
    "## generate the data for the regression\n",
    "x = np.sort(np.random.uniform(0, 20, (50,)))\n",
    "m = 2\n",
    "c = 15\n",
    "# Add on some measurement noise, with std. dev. 3.0\n",
    "epsilon = data = np.random.normal(0, 200, x.shape)\n",
    "y = m * x * x + c + epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now the imputation; we will try and infer missing some missing values of y (we still have the corresponding x)\n",
    "## mark last three values of y invalid\n",
    "y_impute = y[:]\n",
    "\n",
    "n_missing = 6\n",
    "impute_ixs = np.sort(np.random.randint(0, len(y)-1, size=n_missing))\n",
    "y_impute[impute_ixs] = 0\n",
    "y_impute = ma.masked_equal(y_impute,0)\n",
    "print(\"Y masked for imputation:\", y_impute) # we will see the last three entries with --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model (exactly as before, except we switch \"y_impute\" for \"y\")\n",
    "m_unknown = mc.Normal('m', 0, 0.01)\n",
    "c_unknown = mc.Normal('c', 0, 0.001)\n",
    "precision = mc.Gamma('precision', alpha=1, beta=5)\n",
    "std = mc.Lambda('std_dev', lambda precision=precision: np.sqrt(1.0/precision))\n",
    "x_obs = mc.Normal(\"x_obs\", 0, 1, \n",
    "                  value=x, observed=True)\n",
    "\n",
    "@mc.deterministic(plot=False)\n",
    "def line(m=m_unknown, c=c_unknown, x=x_obs):\n",
    "    return x*x*m+c\n",
    "\n",
    "y_obs =  mc.Normal('y_obs', mu=line, \n",
    "                   tau=precision, value=y_impute, \n",
    "                   observed=True)\n",
    "model = mc.Model([m_unknown, c_unknown, std, \n",
    "                  precision, x_obs, y_obs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the distribution\n",
    "mcmc = mc.MCMC(model)\n",
    "mcmc.sample(iter=100000, burn=5000, thin=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "## now we will have three entries in the y_obs trace from this run\n",
    "y_trace = mcmc.trace('y_obs')[:]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "## the original data\n",
    "ax.plot(x, y, '.', label=\"Data\")\n",
    "\n",
    "ax.plot(x, x*x*m+c, ':', label=\"True\")\n",
    "\n",
    "m_sample = mcmc.trace(\"m\")[:]\n",
    "c_sample = mcmc.trace(\"c\")[:]\n",
    "\n",
    "for i in range(20):\n",
    "    m_post = np.random.choice(m_sample)\n",
    "    c_post = np.random.choice(c_sample)\n",
    "    ax.plot(x, x*x*m_post + c_post, \"g\", alpha=0.1,\n",
    "           label=\"Posterior\" if i==0 else None)\n",
    "    \n",
    "\n",
    "\n",
    "# samples from posterior predicted for the missing values of y\n",
    "for i in range(len(impute_ixs)):\n",
    "        \n",
    "    ax.axvline(x[impute_ixs[i]], c='C1', alpha=0.1, label=\"Imputed\" if i==0 else None)\n",
    "    # plot the actual imputed data points\n",
    "    ax.scatter(np.tile(x[impute_ixs[i]], \n",
    "                        (len(y_trace), 1)), \n",
    "                        y_trace[:,i], s=2, c='C3', marker='_', \n",
    "                        alpha=0.25)\n",
    "              \n",
    "    # uncomment to add box plots\n",
    "    #ax.boxplot([y_trace[:,i]], positions = \n",
    "    #           [x[impute_ixs[i]]], widths=2, bootstrap=200,\n",
    "    #          notch=True, showfliers=False )\n",
    "\n",
    "ax.set_xlim(-1,25)\n",
    "ax.set_xticks(np.arange(0,25,5))\n",
    "ax.set_xticklabels(np.arange(0,25,5))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trace(mcmc, {\"std_dev\":\"Standard deviation\", \"m\":\"m\", \"c\":\"c\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext_format_version": "1.0",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
